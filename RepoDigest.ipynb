{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI/rv+pYYGI22bEp7A2FHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HirotoShioi/repo-digest-tool/blob/main/RepoDigest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repo Digest Tool\n",
        "\n",
        "Extract and summarize contents from GitHub repositories with advanced filtering options, tailored for LLM data preparation—all within Google Colab.\n",
        "\n",
        "## Description\n",
        "\n",
        "This tool allows you to clone a GitHub repository, filter its files based on specified criteria, aggregate their contents, and download the combined result—all from within a Google Colab environment. It's particularly useful for preparing data for Large Language Models (LLMs) by extracting relevant code and documentation from repositories.\n",
        "\n",
        "## Features\n",
        "\n",
        "- **Clone Repositories**: Clone public or private GitHub repositories directly in Colab.\n",
        "- **Advanced Filtering**:\n",
        "  - Filter files by target directories and file extensions.\n",
        "  - Ignore specific files and directories based on patterns.\n",
        "- **Content Aggregation**: Combine the contents of filtered files into a single text file.\n",
        "- **Easy Download**: Automatically download the aggregated file within the Colab interface.\n",
        "- **Customizable**: Adjust parameters to suit different repositories and requirements."
      ],
      "metadata": {
        "id": "tk_Xm1Dmnutb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tx35RxJcnoQN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import fnmatch\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from typing import Optional, List\n",
        "from google.colab import files\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def download_repo(repo_url: str, repo_id: str, github_token: Optional[str] = None, branch: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Clone the specified GitHub repository.\n",
        "    \"\"\"\n",
        "    if not repo_url.startswith(\"https://github.com/\"):\n",
        "        raise ValueError(\"Invalid GitHub URL. Please provide a valid GitHub repository URL.\")\n",
        "\n",
        "    # Ensure tmp directory exists\n",
        "    os.makedirs(\"tmp\", exist_ok=True)\n",
        "\n",
        "    repo_path = f\"tmp/{repo_id}\"\n",
        "    # Check and remove the existing clone directory\n",
        "    if os.path.exists(repo_path):\n",
        "        print(f\"Cleaning up existing directory: {repo_path}\")\n",
        "        shutil.rmtree(repo_path)\n",
        "\n",
        "    cmd = [\"git\", \"clone\", \"--depth=1\"]\n",
        "    if branch:\n",
        "        cmd.extend([\"--branch\", branch])\n",
        "\n",
        "    if github_token:\n",
        "        # Embed the token into the URL\n",
        "        repo_url = repo_url.replace(\n",
        "            \"https://github.com/\", f\"https://{github_token}:x-oauth-basic@github.com/\"\n",
        "        )\n",
        "    cmd.append(repo_url)\n",
        "    cmd.append(repo_path)  # Specify the clone destination directory\n",
        "\n",
        "    # Execute the command\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise RuntimeError(f\"Failed to clone the repository: {e.stderr or e}\")\n",
        "\n",
        "\n",
        "def process_repo(\n",
        "    repo_id: str,\n",
        "    target_dir: Optional[str] = None,\n",
        "    extensions: Optional[List[str]] = None,\n",
        "    ignore_files: Optional[List[str]] = None,\n",
        "    ignore_dirs: Optional[List[str]] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Filter files in the repository based on specified conditions and combine them into a single file.\n",
        "    \"\"\"\n",
        "    repo_path = f\"tmp/{repo_id}\"\n",
        "    if not os.path.exists(repo_path):\n",
        "        raise ValueError(f\"Repository path '{repo_path}' does not exist.\")\n",
        "\n",
        "    # Set default values\n",
        "    extensions = extensions or []\n",
        "    ignore_files = ignore_files or []\n",
        "    ignore_dirs = ignore_dirs or []\n",
        "\n",
        "    # Process target patterns\n",
        "    if isinstance(target_dir, str):\n",
        "        target_patterns = [target_dir.strip()]\n",
        "    elif isinstance(target_dir, list):\n",
        "        target_patterns = [pattern.strip() for pattern in target_dir]\n",
        "    else:\n",
        "        target_patterns = [\"**\"]  # Default to target the entire repository\n",
        "\n",
        "    # Expand ignore directory patterns\n",
        "    ignore_dir_paths = set()\n",
        "    for ignore_pattern in ignore_dirs:\n",
        "        full_ignore_pattern = os.path.join(repo_path, ignore_pattern.strip())\n",
        "        ignore_dir_paths.update(glob.glob(full_ignore_pattern, recursive=True))\n",
        "\n",
        "    # File filtering\n",
        "    filtered_files = []\n",
        "    for pattern in target_patterns:\n",
        "        full_pattern = os.path.join(repo_path, pattern)\n",
        "        for file_path in glob.glob(full_pattern, recursive=True):\n",
        "            if os.path.isfile(file_path):\n",
        "                relative_path = os.path.relpath(file_path, repo_path)\n",
        "                # Check if the file is in an ignored directory\n",
        "                if any(os.path.commonpath([file_path, ignore_dir]) == ignore_dir for ignore_dir in ignore_dir_paths):\n",
        "                    continue\n",
        "                # Check if the file matches any ignore file patterns\n",
        "                if any(fnmatch.fnmatch(relative_path, pattern) for pattern in ignore_files):\n",
        "                    continue\n",
        "                # Filter by extensions\n",
        "                if not extensions or any(fnmatch.fnmatch(relative_path, pattern) for pattern in extensions):\n",
        "                    filtered_files.append(file_path)\n",
        "\n",
        "    # Create a list of files\n",
        "    file_list = [os.path.relpath(file_path, repo_path) for file_path in filtered_files]\n",
        "\n",
        "    # Combine file contents\n",
        "    output_content = []\n",
        "    output_content.append(\"\\n\".join(file_list))  # Add the file list at the beginning\n",
        "    for file_path in filtered_files:\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                relative_path = os.path.relpath(file_path, repo_path)\n",
        "                output_content.append(f\"# {relative_path}\\n{f.read()}\\n\")\n",
        "        except Exception as e:\n",
        "            output_content.append(f\"# Error reading file {relative_path}: {e}\\n\")\n",
        "\n",
        "    # Add timestamp to the output file name\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_path = f\"tmp/{repo_id}_digest_{timestamp}.txt\"\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\\n\".join(output_content))\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def download_digest(file_path: str):\n",
        "    \"\"\"\n",
        "    Download the locally generated file.\n",
        "    \"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        files.download(file_path)\n",
        "    else:\n",
        "        print(\"File not found:\", file_path)\n",
        "\n",
        "\n",
        "def main(\n",
        "    repo_url: str,\n",
        "    github_token: Optional[str],\n",
        "    branch: Optional[str] = None,\n",
        "    target_dir: Optional[str] = None,\n",
        "    extensions: Optional[List[str]] = None,\n",
        "    ignore_files: Optional[List[str]] = None,\n",
        "    ignore_dirs: Optional[List[str]] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Main Process\n",
        "    \"\"\"\n",
        "    repo_id = repo_url.split(\"/\")[-1].replace(\".git\", \"\").replace(\"/\", \"_\")\n",
        "\n",
        "    try:\n",
        "        print(\"Cloning repository...\")\n",
        "        download_repo(repo_url, repo_id, github_token, branch)\n",
        "\n",
        "        print(\"Processing repository...\")\n",
        "        digest_path = process_repo(\n",
        "            repo_id,\n",
        "            target_dir=target_dir,\n",
        "            extensions=extensions,\n",
        "            ignore_files=ignore_files,\n",
        "            ignore_dirs=ignore_dirs,\n",
        "        )\n",
        "\n",
        "        print(\"Downloading digest...\")\n",
        "        download_digest(digest_path)\n",
        "\n",
        "        print(\"Cleaning up...\")\n",
        "        shutil.rmtree(f\"tmp/{repo_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete everything berfore it runs\n",
        "!rm -rf /content/tmp/\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    github_token = None  # Set github access token if needed\n",
        "\n",
        "    parameters = {\n",
        "        \"repo_url\": \"https://github.com/HirotoShioi/query-cache\",\n",
        "        \"github_token\": github_token,\n",
        "        \"branch\": None,\n",
        "        \"target_dir\": [\"packages/**\"],\n",
        "        \"extensions\": None,\n",
        "        \"ignore_files\": [\"*.ttf\", \"*.png\", \"pnpm-lock.yaml\", \"*.pdf\", \"*.svg\"],\n",
        "        \"ignore_dirs\": None,\n",
        "    }\n",
        "\n",
        "    main(**parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ZmwJqL0hn_6i",
        "outputId": "a249ed6a-6795-4bb2-e94b-37240add3d24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository...\n",
            "Processing repository...\n",
            "Downloading digest...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a01701b-d845-49da-af93-4c9d7889d194\", \"query-cache_digest_20241202_191952.txt\", 33839)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up...\n"
          ]
        }
      ]
    }
  ]
}